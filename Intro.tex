% - http://sebastianraschka.com/Articles/2014_pca_step_by_step.html




\begin{itemize}
\item Here, our desired outcome of the principal component analysis is to project a feature space (our dataset consisting of 
n x d-dimensional samples) onto a smaller subspace that represents our data "well".
\item A possible application would be a pattern 
classification task, where we want to reduce the computational costs and the error of parameter estimation by reducing the 
number of dimensions of our feature space by extracting a subspace that describes our data "best".
\end{itemize}


%==================================================================================================%

\subsection*{Principal Component Analysis (PCA) Vs. Multiple Discriminant Analysis (MDA)}

Both Multiple Discriminant Analysis (MDA) and Principal Component Analysis (PCA) are linear transformation methods and closely related to each other. In PCA, we are interested to find the directions (components) that maximize the variance in our dataset, where in MDA, we are additionally interested to find the directions that maximize the separation (or discrimination) between different classes (for example, in pattern classification problems where our dataset consists of multiple classes. In contrast two PCA, which ignores the class labels).
In other words, via PCA, we are projecting the entire set of data (without class labels) onto a different subspace, and in MDA, we are trying to determine a suitable subspace to distinguish between patterns that belong to different classes. Or, roughly speaking in PCA we are trying to find the axes with maximum variances where the data is most spread (within a class, since PCA treats the whole data set as one class), and in MDA we are additionally maximizing the spread between classes. 
In typical pattern recognition problems, a PCA is often followed by an MDA.

%=======================================================================================================================%
\end{document}